
# ğŸº Projeto de IngestÃ£o e Modelagem de Dados - Open Brewery DB

Este projeto realiza a ingestÃ£o de dados da API pÃºblica [Open Brewery DB](https://www.openbrewerydb.org/), salvando os dados brutos em PostgreSQL e posteriormente no catÃ¡logo do Databricks, seguindo a arquitetura Medallion (camadas bronze, silver e gold). O pipeline estÃ¡ versionado no GitHub e executado por Jobs no Databricks.

---

## âš™ï¸ Etapa 1 - Infraestrutura local com Docker Compose

Utilizamos um `docker-compose.yml` para levantar dois serviÃ§os:

- **PostgreSQL**: para persistÃªncia inicial dos dados brutos
- **Jupyter Notebook**: ambiente de desenvolvimento em Python

### ğŸ”§ Como subir:

```bash
docker compose up -d
```

---

## ğŸ”„ Etapa 2 â€“ IngestÃ£o com Airbyte (via Docker e `abctl`)

Para facilitar testes locais e reproduzir o pipeline de ingestÃ£o de forma prÃ¡tica, o **Airbyte** foi configurado de duas formas distintas:

### ğŸ”¹ 1. Acesso Ã  documentaÃ§Ã£o oficial

O Airbyte Open Source pode ser explorado em [https://airbyte.com/product/airbyte-open-source](https://airbyte.com/product/airbyte-open-source).

---

### ğŸ”¹ 2. ExecuÃ§Ã£o via Docker (manual)

O Airbyte tambÃ©m foi executado manualmente via Docker (sem uso de `docker-compose`).  
Este modo permite rodar os containers do Airbyte diretamente com o Docker Desktop, sem acoplamento Ã  infraestrutura principal do projeto.

### ğŸ”¹ 3. ExecuÃ§Ã£o via `abctl` (linha de comando)

Seguindo a [documentaÃ§Ã£o oficial](https://docs.airbyte.com/deploying-airbyte/on-your-computer/abctl), o Airbyte tambÃ©m foi instalado via CLI (`abctl`), permitindo uma forma prÃ¡tica e controlada de gerenciar instÃ¢ncias locais:

#### ğŸ“¦ Passos principais:

1. **Verificar arquitetura do sistema**  
   Acesse: `ConfiguraÃ§Ãµes > Sistema > Sobre` e identifique se Ã© `AMD` ou `ARM`.

2. **Download e instalaÃ§Ã£o do `abctl`**  
   FaÃ§a o download da versÃ£o correta para seu sistema operacional, extraia os arquivos e adicione a pasta ao `PATH`.

3. **InstalaÃ§Ã£o do Airbyte local**  
   Com Docker Desktop aberto, rode:

   ```bash
   abctl local install
   ```

4. **Recuperar credenciais padrÃ£o** (usuÃ¡rio/senha para acessar o painel do Airbyte):

   ```bash
   abctl local credentials
   ```

---

### ğŸ”§ ConfiguraÃ§Ã£o do Airbyte:

- **Fonte (source):** PostgreSQL (Docker local)
- **Destino (destination):** Databricks Delta Lake (Unity Catalog)
- **Tabela destino:** `projto_ambev.public.raw_breweries`

Dessa forma, a ingestÃ£o Ã© automatizada e mantida atualizada de forma confiÃ¡vel, servindo como base para o restante do pipeline (bronze â†’ silver â†’ gold).

---

## ğŸ§± Etapa 3 - Arquitetura Medallion (Bronze, Silver, Gold)

### ğŸ”¹ Bronze

- PersistÃªncia do JSON bruto vindo do PostgreSQL
- Tabela: `projto_ambev.bronze.breweries_json`

### ğŸ”¹ Silver

- ConversÃ£o do campo `raw_data` (string JSON) para `STRUCT` usando `from_json`
- ExtraÃ§Ã£o dos principais campos: `id`, `name`, `brewery_type`, `city`, `state`, `latitude`, etc.
- Tabela: `projto_ambev.silver.breweries`

### ğŸ”¹ Gold

AgregaÃ§Ãµes analÃ­ticas:

- Total de cervejarias por estado â†’ `gold.total_por_estado`
- Total por tipo de cervejaria â†’ `gold.total_por_tipo`
- Top 10 cidades com mais cervejarias â†’ `gold.top10_cidades`

---

## ğŸ”„ Etapa 4 - OrquestraÃ§Ã£o com Databricks Jobs

Criamos um Job com mÃºltiplas tasks no Databricks que executa:

1. Notebook Bronze â†’ Silver
2. Notebook Silver â†’ Gold

TambÃ©m Ã© possÃ­vel configurar agendamento e alertas via interface grÃ¡fica.

---

## ğŸ—‚ï¸ Estrutura final no Databricks

```
projto_ambev/
â”œâ”€â”€ public/
â”‚   â””â”€â”€ raw_breweries
â”œâ”€â”€ bronze/
â”‚   â””â”€â”€ breweries_json
â”œâ”€â”€ silver/
â”‚   â””â”€â”€ breweries
â””â”€â”€ gold/
    â”œâ”€â”€ total_por_estado
    â”œâ”€â”€ total_por_tipo
    â””â”€â”€ top10_cidades
```

---

## ğŸ’» Tecnologias utilizadas

- Python 3
- Docker + Docker Compose
- PostgreSQL
- Jupyter Notebook
- Spark (Databricks)
- Delta Lake
- Unity Catalog
- GitHub
- Airbyte
- Databricks Jobs

---

## ğŸ“’ Notebooks do Projeto

Este repositÃ³rio tambÃ©m inclui notebooks utilizados nas etapas do pipeline de dados:

### ğŸ”¹ `Extraindo_dados_brutos.ipynb`
ğŸ“¥ Executado localmente no Jupyter Notebook.  
ResponsÃ¡vel por extrair dados da API [Open Brewery DB](https://www.openbrewerydb.org/) e inserir os registros no banco **PostgreSQL** que estÃ¡ rodando em um container Docker.

### ğŸ”¹ `1_bronze_copia_dados.ipynb`
ğŸ“¦ Notebook executado no **Databricks**.  
LÃª os dados do PostgreSQL e os salva no formato Delta Lake, compondo a camada **Bronze**, onde os dados sÃ£o mantidos em sua forma bruta.

### ğŸ”¹ `2_silver_normalizar_dados.ipynb`
ğŸ§¹ TambÃ©m executado no **Databricks**.  
Transforma os dados brutos da Bronze, usando funÃ§Ãµes como `from_json` e `selectExpr`, para extrair e normalizar os principais campos, criando a camada **Silver**.

### ğŸ”¹ `3_gold_dados_mensurados.ipynb`
ğŸ“Š Notebook final executado no **Databricks**.  
Agrega os dados normalizados da camada Silver em mÃ©tricas e indicadores analÃ­ticos, compondo a camada **Gold**, que serve de base para dashboards e consumo analÃ­tico.

---

## ğŸ§  ObservaÃ§Ãµes TÃ©cnicas - Boas PrÃ¡ticas com Delta Lake no Databricks

Pensando em cenÃ¡rios de Big Data com grandes volumes de dados e mÃºltiplas transformaÃ§Ãµes, algumas boas prÃ¡ticas foram consideradas (ou podem ser implementadas futuramente) no uso do **Delta Lake** no Databricks:

- âœ… **Particionamento de dados**: melhora a performance de leitura e escrita, especialmente em consultas filtradas por colunas temporais como `date`, `ano_mes`, `event_date`, etc.
- ğŸ” **Time Travel (`VERSION AS OF`)**: permite acessar versÃµes anteriores da tabela, Ãºtil para auditoria, debug e rollback de transformaÃ§Ãµes.
- ğŸ§¹ **Vacuum**: remove arquivos antigos e nÃ£o referenciados para economizar armazenamento e manter a performance.

  ```sql
  VACUUM nome_da_tabela RETAIN 168 HOURS;
  ```

- ğŸ“Š **Z-Ordering** (quando aplicÃ¡vel): otimiza a ordenaÃ§Ã£o dos dados internamente, melhorando ainda mais o desempenho de queries.
- ğŸ’¾ **OPTIMIZE**: compacta pequenos arquivos e melhora o desempenho geral de leitura.

Essas prÃ¡ticas sÃ£o fundamentais para manter um **data lakehouse saudÃ¡vel, performÃ¡tico e escalÃ¡vel**.

---

## ğŸ‘¨â€ğŸ’» Autor

Rafael Carlos dos Santos  
Engenheiro de Dados | [LinkedIn](https://www.linkedin.com/in/rafaelcarlossantos/)
